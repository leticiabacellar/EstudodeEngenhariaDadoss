# Apache Spark

Vamos mergulhar no **Apache Spark**, desde os conceitos te√≥ricos at√© como us√°-lo na pr√°tica com Python (PySpark). Voc√™ j√° tem uma boa base em dados, ent√£o vou te explicar de forma clara, mas completa.

---

## üî• O que √© o Apache Spark?

O **Apache Spark** √© um mecanismo de processamento de dados de c√≥digo aberto, projetado para ser:

- **R√°pido** (at√© 100x mais r√°pido que o Hadoop MapReduce)
- **Escal√°vel** (funciona em clusters locais ou na nuvem)
- **Vers√°til** (funciona com batch, streaming, machine learning e SQL)

---

## üß† Conceitos Te√≥ricos

### 1. **Componentes principais**

| Componente      | Fun√ß√£o                                            |
| --------------- | ------------------------------------------------ |
| **Spark Core**  | Gerencia tarefas b√°sicas: agendamento, mem√≥ria, falhas. |
| **Spark SQL**   | Permite usar SQL para consultar dados.           |
| **Spark Streaming** | Processa dados em tempo real.                   |
| **MLlib**       | Biblioteca de machine learning.                   |
| **GraphX**      | Trabalha com grafos.                              |

---

### 2. **RDD vs DataFrame vs Dataset**

| Tipo                  | Explica√ß√£o                                                                                 |
| --------------------- | ----------------------------------------------------------------------------------------- |
| **RDD** (Resilient Distributed Dataset) | √â a base de dados distribu√≠dos do Spark. Imut√°vel, tolerante a falhas, mas de mais baixo n√≠vel. |
| **DataFrame**          | Tabela distribu√≠da com colunas nomeadas. Mais eficiente e f√°cil de usar. Baseado em SQL.   |
| **Dataset**            | Combina os benef√≠cios de RDDs (forte tipagem) com DataFrames (otimiza√ß√£o). Mais usado em Scala/Java. |

---

### 3. **Lazy Evaluation**

Spark **n√£o executa imediatamente** as opera√ß√µes. Ele monta um plano de execu√ß√£o e s√≥ executa quando uma "a√ß√£o" √© chamada, como `.show()`, `.collect()`, `.write()`. Isso melhora o desempenho.

---

## üõ†Ô∏è Pr√°tica com PySpark

### üß± 1. Criando uma sess√£o Spark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Meu Projeto Spark") \
    .getOrCreate()
