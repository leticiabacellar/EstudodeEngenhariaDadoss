# Apache Spark

Vamos mergulhar no **Apache Spark**, desde os conceitos teÃ³ricos atÃ© como usÃ¡-lo na prÃ¡tica com Python (PySpark). VocÃª jÃ¡ tem uma boa base em dados, entÃ£o vou te explicar de forma clara, mas completa.

---

## ğŸ”¥ O que Ã© o Apache Spark?

O **Apache Spark** Ã© um mecanismo de processamento de dados de cÃ³digo aberto, projetado para ser:

- **RÃ¡pido** (atÃ© 100x mais rÃ¡pido que o Hadoop MapReduce)
- **EscalÃ¡vel** (funciona em clusters locais ou na nuvem)
- **VersÃ¡til** (funciona com batch, streaming, machine learning e SQL)

---

## ğŸ§  Conceitos TeÃ³ricos

### 1. **Componentes principais**

| Componente      | FunÃ§Ã£o                                            |
| --------------- | ------------------------------------------------ |
| **Spark Core**  | Gerencia tarefas bÃ¡sicas: agendamento, memÃ³ria, falhas. |
| **Spark SQL**   | Permite usar SQL para consultar dados.           |
| **Spark Streaming** | Processa dados em tempo real.                   |
| **MLlib**       | Biblioteca de machine learning.                   |
| **GraphX**      | Trabalha com grafos.                              |

---

### 2. **RDD vs DataFrame vs Dataset**

| Tipo                  | ExplicaÃ§Ã£o                                                                                 |
| --------------------- | ----------------------------------------------------------------------------------------- |
| **RDD** (Resilient Distributed Dataset) | Ã‰ a base de dados distribuÃ­dos do Spark. ImutÃ¡vel, tolerante a falhas, mas de mais baixo nÃ­vel. |
| **DataFrame**          | Tabela distribuÃ­da com colunas nomeadas. Mais eficiente e fÃ¡cil de usar. Baseado em SQL.   |
| **Dataset**            | Combina os benefÃ­cios de RDDs (forte tipagem) com DataFrames (otimizaÃ§Ã£o). Mais usado em Scala/Java. |

---

### 3. **Lazy Evaluation**

Spark **nÃ£o executa imediatamente** as operaÃ§Ãµes. Ele monta um plano de execuÃ§Ã£o e sÃ³ executa quando uma "aÃ§Ã£o" Ã© chamada, como `.show()`, `.collect()`, `.write()`. Isso melhora o desempenho.

---

## ğŸ› ï¸ PrÃ¡tica com PySpark

### ğŸ§± 1. Criando uma sessÃ£o Spark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Meu Projeto Spark") \
    .getOrCreate()
```


###  ğŸ“‚ 2. Lendo dados (CSV, Parquet, JSON etc.)

```python
df = spark.read.option("header", True).csv("arquivo.csv")
```

### ğŸ” 3. Visualizando e manipulando dados

```python
df.show()
df.printSchema()
df.select("coluna1", "coluna2").filter("coluna1 > 100").show()
```

### ğŸ§ª 4. TransformaÃ§Ãµes e agregaÃ§Ãµes

```python
from pyspark.sql.functions import col, avg

df_filtered = df.filter(col("vendas") > 1000)
media = df_filtered.groupBy("categoria").agg(avg("vendas").alias("media_vendas"))
media.show()
```

### ğŸ“¤ 5. Exportando os resultados

```python
media.write.mode("overwrite").option("header", True).csv("saida/media_vendas")
```

### ğŸ’¥ 6. Encerrando a sessÃ£o

```python
spark.stop()
```

## âš™ï¸ Spark no mundo real

### Casos de uso:

- Processar milhÃµes de linhas de logs (como vocÃª estÃ¡ fazendo)
- Limpar e transformar dados para dashboards (ETL)
- Machine Learning distribuÃ­do (com MLlib)
- Stream de dados com Kafka, Spark Streaming

---

## ğŸ§  BenefÃ­cios do Spark

- Processamento **distribuÃ­do** (usa vÃ¡rios nÃºcleos/servidores)
- Suporte a **dados estruturados** e **nÃ£o estruturados**
- Pode rodar localmente ou em clusters com **YARN**, **Kubernetes** ou **EMR (AWS)**

---

## ğŸ“Œ Dicas prÃ¡ticas

- Teste localmente com arquivos pequenos (`master="local[*]"`)
- Use `cache()` ou `persist()` se precisar reaproveitar dados
- Evite usar `.collect()` em grandes volumes (puxa tudo para a memÃ³ria)

---

## ğŸ¥ VÃ­deos recomendados

- https://youtu.be/x9geexcW3UU?si=OXrxdOkyKsLK_5_I  
- https://youtu.be/CPYjUA2UNq8?si=kK48eiGkOJYv91P8  
- https://www.youtube.com/watch?v=VfpXMuwbQXc&list=PLjwVjYMyoFoFO-TvZuxmqe5nlfr8xG5lk  
- https://youtu.be/WwrX1YVmOyA?si=7OsGq0SWks_NUJcz  
