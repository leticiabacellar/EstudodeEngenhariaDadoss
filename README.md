# Apache Spark

Vamos mergulhar no **Apache Spark**, desde os conceitos teÃ³ricos atÃ© como usÃ¡-lo na prÃ¡tica com Python (PySpark). VocÃª jÃ¡ tem uma boa base em dados, entÃ£o vou te explicar de forma clara, mas completa.

---

## ğŸ”¥ O que Ã© o Apache Spark?

O **Apache Spark** Ã© um mecanismo de processamento de dados de cÃ³digo aberto, projetado para ser:

- **RÃ¡pido** (atÃ© 100x mais rÃ¡pido que o Hadoop MapReduce)
- **EscalÃ¡vel** (funciona em clusters locais ou na nuvem)
- **VersÃ¡til** (funciona com batch, streaming, machine learning e SQL)

---

## ğŸ§  Conceitos TeÃ³ricos

### 1. **Componentes principais**

| Componente      | FunÃ§Ã£o                                            |
| --------------- | ------------------------------------------------ |
| **Spark Core**  | Gerencia tarefas bÃ¡sicas: agendamento, memÃ³ria, falhas. |
| **Spark SQL**   | Permite usar SQL para consultar dados.           |
| **Spark Streaming** | Processa dados em tempo real.                   |
| **MLlib**       | Biblioteca de machine learning.                   |
| **GraphX**      | Trabalha com grafos.                              |

---

### 2. **RDD vs DataFrame vs Dataset**

| Tipo                  | ExplicaÃ§Ã£o                                                                                 |
| --------------------- | ----------------------------------------------------------------------------------------- |
| **RDD** (Resilient Distributed Dataset) | Ã‰ a base de dados distribuÃ­dos do Spark. ImutÃ¡vel, tolerante a falhas, mas de mais baixo nÃ­vel. |
| **DataFrame**          | Tabela distribuÃ­da com colunas nomeadas. Mais eficiente e fÃ¡cil de usar. Baseado em SQL.   |
| **Dataset**            | Combina os benefÃ­cios de RDDs (forte tipagem) com DataFrames (otimizaÃ§Ã£o). Mais usado em Scala/Java. |

---

### 3. **Lazy Evaluation**

Spark **nÃ£o executa imediatamente** as operaÃ§Ãµes. Ele monta um plano de execuÃ§Ã£o e sÃ³ executa quando uma "aÃ§Ã£o" Ã© chamada, como `.show()`, `.collect()`, `.write()`. Isso melhora o desempenho.

---

## ğŸ› ï¸ PrÃ¡tica com PySpark

### ğŸ§± 1. Criando uma sessÃ£o Spark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Meu Projeto Spark") \
    .getOrCreate()
```


###  ğŸ“‚ 2. Lendo dados (CSV, Parquet, JSON etc.)

```python
df = spark.read.option("header", True).csv("arquivo.csv")
```

### ğŸ” 3. Visualizando e manipulando dados

```python
df.show()
df.printSchema()
df.select("coluna1", "coluna2").filter("coluna1 > 100").show()
```

### ğŸ§ª 4. TransformaÃ§Ãµes e agregaÃ§Ãµes

```python
from pyspark.sql.functions import col, avg

df_filtered = df.filter(col("vendas") > 1000)
media = df_filtered.groupBy("categoria").agg(avg("vendas").alias("media_vendas"))
media.show()
```

### ğŸ“¤ 5. Exportando os resultados

```python
media.write.mode("overwrite").option("header", True).csv("saida/media_vendas")
```

### ğŸ’¥ 6. Encerrando a sessÃ£o

```python
spark.stop()
```

## âš™ï¸ Spark no mundo real

### Casos de uso:

- Processar milhÃµes de linhas de logs (como vocÃª estÃ¡ fazendo)
- Limpar e transformar dados para dashboards (ETL)
- Machine Learning distribuÃ­do (com MLlib)
- Stream de dados com Kafka, Spark Streaming

---

## ğŸ§  BenefÃ­cios do Spark

- Processamento **distribuÃ­do** (usa vÃ¡rios nÃºcleos/servidores)
- Suporte a **dados estruturados** e **nÃ£o estruturados**
- Pode rodar localmente ou em clusters com **YARN**, **Kubernetes** ou **EMR (AWS)**

---

## ğŸ“Œ Dicas prÃ¡ticas

- Teste localmente com arquivos pequenos (`master="local[*]"`)
- Use `cache()` ou `persist()` se precisar reaproveitar dados
- Evite usar `.collect()` em grandes volumes (puxa tudo para a memÃ³ria)

---

## ğŸ¥ VÃ­deos recomendados

- https://youtu.be/x9geexcW3UU?si=OXrxdOkyKsLK_5_I  
- https://youtu.be/CPYjUA2UNq8?si=kK48eiGkOJYv91P8  
- https://www.youtube.com/watch?v=VfpXMuwbQXc&list=PLjwVjYMyoFoFO-TvZuxmqe5nlfr8xG5lk  
- https://youtu.be/WwrX1YVmOyA?si=7OsGq0SWks_NUJcz





# **Apache** Hadoop

Vamos agora entender **o que Ã© o Hadoop**, desde a teoria atÃ© a prÃ¡tica, com foco em como ele funciona, para que serve e como vocÃª pode usÃ¡-lo (inclusive comparando com o Spark quando for Ãºtil).

---

## ğŸ§± O que Ã© o Apache Hadoop?

O **Apache Hadoop** Ã© uma **plataforma open-source para processamento e armazenamento distribuÃ­do de grandes volumes de dados**. Foi projetado para escalar de um Ãºnico servidor atÃ© milhares de mÃ¡quinas.

---

## ğŸ§  Teoria: Componentes do Hadoop

### 1. **HDFS (Hadoop Distributed File System)**

- Sistema de arquivos distribuÃ­do: divide os arquivos em blocos (geralmente 128 MB ou 256 MB).
- Armazena cÃ³pias dos blocos em **vÃ¡rios nÃ³s** para garantir **tolerÃ¢ncia a falhas**.

### 2. **YARN (Yet Another Resource Negotiator)**

- Gerencia os **recursos** e **agendamento** das tarefas.
- Atua como um â€œsistema operacionalâ€ do cluster.

### 3. **MapReduce**

- Modelo de programaÃ§Ã£o do Hadoop para **processar dados em paralelo**.
- **Map** â†’ transforma os dados (ex: filtrar, agrupar).
- **Reduce** â†’ agrega os resultados (ex: somar, contar).

### 4. **Hadoop Common**

- Biblioteca com utilitÃ¡rios usados por outros mÃ³dulos Hadoop.

---

## ğŸ“¦ Exemplo de arquitetura Hadoop

+---------------------+
| Cliente / UsuÃ¡rio |
+---------------------+
|
v
+---------------------+
| YARN (JobTracker)|
+---------------------+
|
v
+---------------------+
| MapReduce |
+---------------------+
|
v
+---------------------+
| HDFS |
+--------+------------+
|
+------+-------+
| Bloco 1 |
| Bloco 2 |
| Bloco 3 |
+--------------+


---

## ğŸ› ï¸ Como usar Hadoop na prÃ¡tica

### ğŸ’» 1. Instalar Hadoop (modo local para estudos)

- Pode usar uma VM ou Docker
- Modo *pseudo-distribuÃ­do* (simula um cluster em uma mÃ¡quina)

### ğŸ“ 2. Subir arquivos no HDFS

```bash
hdfs dfs -mkdir /dados
hdfs dfs -put local_arquivo.csv /dados/
```

### ğŸ—ºï¸ 3. Rodar um job MapReduce (modo clÃ¡ssico)

```bash
hadoop jar wordcount.jar org.apache.hadoop.examples.WordCount /dados /saida
```

---

Mas hoje em dia, vocÃª provavelmente vai usar ferramentas que rodam sobre o Hadoop, como:

| Ferramenta | Para quÃª serve                      |
| ---------- | --------------------------------- |
| **Hive**   | SQL sobre Hadoop                  |
| **Pig**    | Script sobre Hadoop               |
| **Spark**  | Processamento rÃ¡pido (roda sobre HDFS/YARN) |
| **HBase**  | Banco NoSQL                      |
| **Sqoop**  | Importar/exportar dados entre bancos e HDFS |

---

## ğŸ§ª Exemplo simples: Word Count com Hadoop Streaming

VocÃª pode usar **Python com MapReduce** assim:

### mapper.py

```python
import sys
for linha in sys.stdin:
    for palavra in linha.strip().split():
        print(f"{palavra}\t1")
```


### reducer.py

```python
import sys
from collections import defaultdict

contagem = defaultdict(int)
for linha in sys.stdin:
    palavra, valor = linha.strip().split("\t")
    contagem[palavra] += int(valor)

for palavra, total in contagem.items():
    print(f"{palavra}\t{total}")
```

### Rodar com Hadoop:

```bash
hadoop jar hadoop-streaming.jar \
  -input /dados \
  -output /saida \
  -mapper mapper.py \
  -reducer reducer.py
```

## ğŸ“Š Quando usar Hadoop?

| SituaÃ§Ã£o                        | Hadoop | Spark |
| ------------------------------ | ------ | ----- |
| Processar dados MUITO grandes (PB) | âœ…    | âœ…    |
| Processamento mais rÃ¡pido       | ğŸš«    | âœ…    |
| Streaming de dados             | ğŸš«    | âœ…    |
| Custo de cluster menor         | âœ…    | ğŸš«    |
| Machine Learning               | ğŸš«    | âœ…    |

---

## ğŸš€ Resumo das Vantagens do Hadoop

- Armazenamento **distribuÃ­do e seguro**
- Alta **tolerÃ¢ncia a falhas**
- IntegraÃ§Ã£o com muitas ferramentas Big Data
- **Open-source** e amplamente adotado

---

## ğŸ§© Curiosidade: Hadoop x Spark

- Spark pode rodar sobre o Hadoop usando HDFS para armazenamento e YARN como orquestrador.
- Ou seja, eles nÃ£o sÃ£o concorrentes diretos, e sim complementares.

---

## ğŸ¥ VÃ­deos recomendados

- https://youtu.be/pcAdeZ5fLB8?si=z1TzOSMEzbLda232  
- https://www.youtube.com/watch?v=NmzNlov-HOI&list=PLFJLvHW_AAoCgaQLMu9tgYyOtc0m3-tDV  








# Apache NiFi

Vamos falar agora sobre o **Apache NiFi**, uma ferramenta super Ãºtil principalmente para quem trabalha com **dados em movimento (streaming ou ETL)**.

---

## ğŸŒŠ O que Ã© o Apache NiFi?

O **Apache NiFi** Ã© uma **ferramenta grÃ¡fica para automaÃ§Ã£o de fluxo de dados**. Ele permite que vocÃª **movimente, transforme, integre e monitore dados entre sistemas**, de forma simples e visual.

---

## ğŸ§  Conceito TeÃ³rico

### ğŸ§© O que o NiFi faz?

- Coleta dados de mÃºltiplas fontes
- Faz transformaÃ§Ãµes nos dados
- Envia os dados para outros sistemas (bancos, APIs, nuvens, etc.)
- Tudo isso com um **fluxo visual (drag-and-drop)**

---

### âš™ï¸ Componentes principais

| Componente | FunÃ§Ã£o |
| --- | --- |
| **Processor** | Bloco que executa uma tarefa (ex: `GetFile`, `PutSQL`, `UpdateAttribute`) |
| **FlowFile** | Ã‰ o "pacote de dados" que circula nos fluxos. Tem conteÃºdo + atributos |
| **Connection** | Liga os Processors e controla o fluxo (buffers, prioridades) |
| **Controller Service** | Configura conexÃµes externas (bancos, APIs etc.) |
| **Process Group** | Um agrupamento de Processors (tipo uma "caixinha" de ETL) |
| **Templates** | Fluxos que podem ser exportados/importados |

---

### ğŸ“ Como funciona o fluxo

1. **Entrada**: Coleta arquivos, dados de APIs, bancos, Kafka, etc.
2. **Processamento**: Filtra, converte, transforma, adiciona campos.
3. **SaÃ­da**: Envia os dados para bancos, S3, outro sistema, etc.

---

## ğŸ§ª Exemplo prÃ¡tico simples

### ğŸ¯ Objetivo:

Mover dados de um arquivo CSV local para um banco de dados MySQL.

### âš™ï¸ Processadores usados:

1. `GetFile` â€“ LÃª o arquivo CSV
2. `UpdateAttribute` â€“ Renomeia o arquivo (opcional)
3. `ConvertRecord` â€“ Converte de CSV para JSON ou Avro
4. `PutDatabaseRecord` â€“ Insere os dados no banco MySQL

---

## ğŸ§‘â€ğŸ’» Como usar o NiFi na prÃ¡tica

### ğŸ› ï¸ 1. InstalaÃ§Ã£o

VocÃª pode rodar localmente:

```bash
bash
CopiarEditar
wget https://downloads.apache.org/nifi/current/nifi-*-bin.zip
unzip nifi-*.zip
cd nifi-*/bin
./nifi.sh start

```

Acesse no navegador:

```
bash
CopiarEditar
http://localhost:8080/nifi

```

---

### ğŸ§± 2. Montando um fluxo

Na interface grÃ¡fica:

1. Arraste o `GetFile`
2. Configure o caminho da pasta de entrada
3. Conecte ao `ConvertRecord` (para tratar o CSV)
4. Conecte ao `PutDatabaseRecord` (conexÃ£o com banco)
5. Configure a conexÃ£o JDBC no Controller Services

---

### ğŸ“ˆ Monitoramento

- Cada Processador mostra: quantidade de arquivos, bytes processados, taxa de erro
- Logs detalhados em cada FlowFile

---

### ğŸ” SeguranÃ§a

- Criptografia (TLS)
- Controle de acesso (usuÃ¡rios, grupos, permissÃµes)
- Versionamento e auditoria de fluxos

---

## ğŸ§© Conectores prontos

O NiFi jÃ¡ vem com mais de **280 processadores**. Exemplos:

| Fonte/Destino | Processador |
| --- | --- |
| Arquivos locais | `GetFile`, `PutFile` |
| APIs REST | `InvokeHTTP` |
| Banco de dados | `ExecuteSQL`, `PutDatabase` |
| Kafka | `ConsumeKafka`, `PublishKafka` |
| S3, GCS, Azure Blob | `FetchS3Object`, `PutGCSObject` |
| Email | `GetSMTP`, `PutEmail` |

---

## ğŸ§  Quando usar o NiFi?

Use quando vocÃª quer:

âœ… Criar ETLs visuais

âœ… Fazer ingestÃ£o de dados em tempo real

âœ… Monitorar o fluxo de dados com facilidade

âœ… Orquestrar processos com vÃ¡rios sistemas

âœ… Integrar APIs, bancos e arquivos sem cÃ³digo complexo

---

## âš¡ Comparativo com outras ferramentas

| Ferramenta | Foco principal | Visual? | Tempo real? |
| --- | --- | --- | --- |
| Apache NiFi | Fluxo de dados | âœ… | âœ… |
| Airflow | OrquestraÃ§Ã£o de tarefas | âŒ (mais tÃ©cnico) | âŒ (batch) |
| Apache Spark | Processamento em memÃ³ria | âŒ | âœ… (com Structured Streaming) |
| Talend, Pentaho | ETL tradicional | âœ… | âŒ |

---

## ğŸ’¡ Projeto prÃ¡tico sugerido

**Pipeline com NiFi:**

- Entrada: pasta com arquivos `.csv` de vendas
- Processamento: filtrar colunas e converter para JSON
- SaÃ­da: inserir os dados em um banco MySQL ou PostgreSQL

https://www.youtube.com/watch?v=UJG0zj1rPbY&list=PLeblJhqzZe1rszn0z3wqE5ETilASaJoUX

https://youtu.be/wDn7GwF-UYQ?si=uXQOvdYFQnZU1M1p

https://youtu.be/xPtUcHn3qFI?si=7YGtSyoqbxPDHurF
